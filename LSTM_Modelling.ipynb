{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment : 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Preprocess all the Data we have in DonorsChoose <a href='https://drive.google.com/drive/folders/1MIwK7BQMev8f5CbDDVNLPaFGB32pFN60'>Dataset</a> use train.csv\n",
    "2. Combine 4 essay's into one column named - 'preprocessed_essays'. \n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a href='https://datascience.stackexchange.com/a/20192'>this</a> for using auc as a metric \n",
    "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum, resources: <a href='http://cs231n.github.io/neural-networks-3/'>cs231n class notes</a>, <a href='https://www.youtube.com/watch?v=hd_KFJ5ktUc'>cs231n class video</a>. \n",
    "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in .ipynb notebook and PDF. \n",
    "8. Use Categorical Cross Entropy as Loss to minimize.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , StandardScaler , OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input , Dropout,Flatten,concatenate,LSTM ,Embedding, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping , TensorBoard\n",
    "#https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "import datetime, os\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Preprocessing\n",
    "    1.Load Word Embedding\n",
    "    2.Load dataset and segregate dependent and independent fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "Y = data['project_is_approved'].values\n",
    "X = data.drop(['project_is_approved'],axis = 1)\n",
    "# 70-30 split of data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,stratify=Y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "\n",
       "    clean_categories                 clean_subcategories  \\\n",
       "0       math_science  appliedsciences health_lifescience   \n",
       "1       specialneeds                        specialneeds   \n",
       "2  literacy_language                            literacy   \n",
       "3    appliedlearning                    earlydevelopment   \n",
       "4  literacy_language                            literacy   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  \n",
       "2  having class 24 students comes diverse learner...  329.00  \n",
       "3  i recently read article giving students choice...  481.04  \n",
       "4  my students crave challenge eat obstacles brea...   17.74  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating Dependent and Independent features\n",
    "\n",
    "Y = data['project_is_approved'].values\n",
    "X = data.drop(['project_is_approved'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70-30 split of data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,stratify=Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check dimensions of Train and test data\n",
    "print(X_train.shape[0] == y_train.shape[0])\n",
    "print(X_test.shape[0] == y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-1\n",
    "\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For LSTM, you can choose your sequence padding methods on your own or you can train your LSTM without padding, there is no restriction on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "\n",
    "input_layer = Input(shape=(n,))\n",
    "\n",
    "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
    "\n",
    "flatten = Flatten()(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "####  2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Essay feature  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Fit text tokenizer on essay feature of train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "#https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "    \n",
    "#Train tokenizer on Train data only\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(X_train['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chitresh/miniconda3/envs/new_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/chitresh/miniconda3/envs/new_env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train['essay_tokens'] = t.texts_to_sequences(np.asarray(X_train['essay']))\n",
    "X_test['essay_tokens'] = t.texts_to_sequences(np.asarray(X_test['essay']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>essay_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103211</th>\n",
       "      <td>tx</td>\n",
       "      <td>teacher</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl literacy</td>\n",
       "      <td>i teach title i low income school the percenta...</td>\n",
       "      <td>117.04</td>\n",
       "      <td>[2, 49, 112, 2, 91, 134, 3, 7, 1336, 846, 915,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25113</th>\n",
       "      <td>wa</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literature_writing</td>\n",
       "      <td>i diverse group students classroom my students...</td>\n",
       "      <td>208.07</td>\n",
       "      <td>[2, 105, 61, 1, 6, 4, 1, 1068, 91, 134, 118, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106946</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>15</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy literature_writing</td>\n",
       "      <td>my students imaginative creative eager third g...</td>\n",
       "      <td>313.75</td>\n",
       "      <td>[4, 1, 2141, 143, 94, 358, 122, 271, 21, 31, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       school_state teacher_prefix project_grade_category  \\\n",
       "103211           tx        teacher          grades_prek_2   \n",
       "25113            wa             ms          grades_prek_2   \n",
       "106946           ga            mrs             grades_3_5   \n",
       "\n",
       "        teacher_number_of_previously_posted_projects   clean_categories  \\\n",
       "103211                                             2  literacy_language   \n",
       "25113                                              1  literacy_language   \n",
       "106946                                            15  literacy_language   \n",
       "\n",
       "                clean_subcategories  \\\n",
       "103211                 esl literacy   \n",
       "25113            literature_writing   \n",
       "106946  literacy literature_writing   \n",
       "\n",
       "                                                    essay   price  \\\n",
       "103211  i teach title i low income school the percenta...  117.04   \n",
       "25113   i diverse group students classroom my students...  208.07   \n",
       "106946  my students imaginative creative eager third g...  313.75   \n",
       "\n",
       "                                             essay_tokens  \n",
       "103211  [2, 49, 112, 2, 91, 134, 3, 7, 1336, 846, 915,...  \n",
       "25113   [2, 105, 61, 1, 6, 4, 1, 1068, 91, 134, 118, 5...  \n",
       "106946  [4, 1, 2141, 143, 94, 358, 122, 271, 21, 31, 2...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2 Make length of this feature equal to 300 as of our word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_embeddings = 300\n",
    "X_train_essay = pad_sequences(X_train['essay_tokens'].values, maxlen=len_embeddings)\n",
    "X_test_essay  = pad_sequences(X_test['essay_tokens'].values, maxlen=len_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Get the embedding matrix as is required in Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take only those word vectors which are present in our training data\n",
    "\n",
    "#print(type(t.word_index)) - dictionary\n",
    "with open('glove_vectors','rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "    \n",
    "max_words = len(t.word_index)+1\n",
    "embeddings_mat = np.zeros((max_words,len_embeddings))\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_mat[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.3513e-01  4.0743e-01  2.2165e-01 -6.0927e-01 -2.7424e-01 -2.3149e-01\n",
      " -3.2341e+00  3.0772e-01 -1.7940e-02 -7.4481e-01  1.6566e-01  4.1989e-02\n",
      " -1.5045e-01 -4.4603e-01 -1.7743e-01 -1.7807e-01  1.9957e-01  9.8679e-02\n",
      " -9.4911e-01  1.9601e-01  5.3195e-01  5.4851e-01  3.5127e-01  3.5170e-01\n",
      "  2.1292e-02 -5.0950e-01  4.7914e-01  2.5772e-01  4.5941e-01 -7.0546e-02\n",
      "  1.5776e-01  1.5541e-01  4.0705e-01 -4.7664e-01 -2.2956e-01  5.7266e-01\n",
      "  1.2679e-01  1.5115e-01  2.6836e-01  5.8877e-03  2.4412e-01  1.8188e-01\n",
      " -3.4022e-02  3.3863e-01 -1.7361e-01  5.6732e-01 -1.6217e-01 -5.1171e-01\n",
      " -3.4799e-01  4.3382e-01  1.2042e-02 -3.0455e-01 -6.1643e-01  4.6887e-01\n",
      "  7.4270e-01 -4.5566e-01 -7.1125e-03 -1.5474e-01  4.7934e-01  4.1385e-01\n",
      "  3.6900e-02  3.1015e-02  2.4902e-01 -4.5375e-01 -2.7680e-01  7.4268e-02\n",
      "  3.0333e-01  2.7293e-01  1.3760e-01  4.5720e-01 -1.6398e-01  1.2128e-01\n",
      " -9.4683e-01  1.4540e-01  5.0068e-01  6.5366e-03 -2.5565e-01  2.4079e-01\n",
      "  9.5816e-02 -1.4168e-01  2.7307e-02  1.8647e-01 -1.3683e-01 -1.8652e-01\n",
      "  1.7833e-01  2.5942e-02  4.8106e-01  3.3359e-01 -1.0032e-01  1.9906e-01\n",
      "  1.9020e-01  1.3833e-01  1.1702e-01  1.8402e-01  2.1941e-01 -3.0236e-02\n",
      " -2.1964e+00  4.0594e-01  4.2262e-01  1.6997e-01 -2.6491e-01  3.1122e-02\n",
      " -4.6247e-02  2.8391e-01 -1.5766e-01 -2.5030e-01 -4.3728e-01 -3.2444e-01\n",
      "  2.4145e-01 -3.7850e-02 -1.5036e-02  1.5021e-01 -4.0698e-02  9.8807e-02\n",
      " -1.2210e-02 -1.7851e-01 -4.5475e-02 -6.6512e-02  2.2959e-01 -3.6918e-02\n",
      " -3.0987e-01  1.9266e-01 -1.9504e-01  5.5975e-02 -2.2501e-01 -2.8773e-01\n",
      "  1.2492e-01 -2.3836e-01 -1.7950e-01  4.7097e-02  1.8305e-01 -4.3701e-01\n",
      " -1.2902e-01  5.5249e-02  4.1539e-01  2.3182e-01  3.0378e-01  4.7232e-01\n",
      " -1.3641e-01  6.5096e-01 -3.5831e-01  5.5096e-01  1.2717e-02 -2.2584e-02\n",
      "  1.0710e-01  2.2864e-01 -2.3958e-03  2.5625e-01  4.5102e-01 -2.9152e-01\n",
      "  2.9386e-01  5.0092e-02  2.7579e-01 -5.2015e-01  3.1315e-01  6.4405e-02\n",
      "  3.4111e-02  2.0080e-01  8.0838e-02 -2.0822e-01  4.1363e-01  3.3357e-01\n",
      " -1.1609e-01 -2.3767e-01 -3.0716e-01 -1.4118e-01  2.9122e-01  2.2185e-01\n",
      " -4.8196e-02 -2.0891e-01 -3.7758e-01  2.0568e-01 -1.7723e-01  3.1176e-01\n",
      " -3.8770e-01 -4.7644e-01  1.0027e-01  1.4547e-01  4.6264e-02  1.2301e-01\n",
      " -2.3802e-01 -5.1911e-01  1.9510e-01 -2.5873e-02  1.2448e-01 -2.2575e-01\n",
      "  6.9619e-02 -2.1015e-01 -4.0802e-01  1.9076e-01  2.6347e-01  5.4532e-02\n",
      " -3.4739e-01  1.6449e-01 -3.2107e-01  6.8792e-04  2.8810e-01  1.6659e-01\n",
      " -3.3753e-01  5.1277e-01 -4.9382e-02 -1.4416e-01 -1.6565e-01 -2.5818e-01\n",
      " -3.4665e-01 -1.0816e-01 -2.9532e-01  3.2246e-01  3.3716e-01 -7.4922e-02\n",
      "  3.4942e-01 -3.2604e-01 -8.8338e-02 -1.6713e-01 -2.5336e-01 -3.9149e-02\n",
      " -1.8127e-02 -6.7270e-01  2.8907e-01 -2.5795e-01 -1.3378e-01 -1.7375e-01\n",
      " -3.0262e-01  2.6849e-01 -2.4342e+00 -4.2855e-01  2.1168e-01  5.6168e-02\n",
      " -2.4234e-01 -2.1788e-01  2.4203e-01  2.7002e-01  1.4121e-01  3.4652e-02\n",
      " -7.3735e-02 -2.0691e-01 -2.8706e-01  3.2982e-01 -8.9072e-02 -7.6218e-02\n",
      " -4.1696e-01  3.9210e-01  4.2369e-01  2.7720e-01 -3.2029e-01 -1.9132e-02\n",
      " -7.1272e-01  1.6138e-01  6.3814e-01  1.4233e-01  3.0491e-01 -3.1218e-01\n",
      "  4.8611e-02  3.9175e-01  1.6573e-02 -6.2214e-02 -5.3650e-01  2.5741e-01\n",
      "  2.8489e-01  1.0108e-01 -4.6573e-01  1.9900e-01 -1.4482e-02 -7.5924e-01\n",
      " -5.2105e-01  3.1645e-01 -3.1753e-01  2.7174e-01  2.5182e-01 -7.0792e-02\n",
      " -6.2045e-02 -4.5805e-01 -2.5191e-01  4.3021e-01 -3.8277e-01  1.5204e-01\n",
      " -2.7448e-01  1.4394e-01  6.9169e-01 -4.5875e-01  9.7760e-01 -1.1513e-01\n",
      "  1.5591e-01 -1.8887e-01 -3.4145e-01 -9.8730e-02 -7.0142e-02  8.3776e-01\n",
      "  1.6195e-02  2.2481e-01 -3.8441e-01 -3.2155e-01  2.7429e-01 -1.0012e-01\n",
      "  1.1414e-01 -5.6371e-02 -3.1887e-01 -2.5866e-01 -1.7405e-02  3.0057e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_mat[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features:\n",
    "    1. school_state\n",
    "    2. teacher_prefix\n",
    "    3. clean_categories\n",
    "    4. clean_subcategories\n",
    "### Convert these to nunmeric data using Label Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "# Each input has a embedding layer so we need input and output dimensions \n",
    "def categorical_encoding(X,feature):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(X[feature].values)\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9\n",
    "def len_embedding(X):\n",
    "    l = min(np.ceil(len(np.unique(X))/2),50)\n",
    "    return int(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of school_state embedding is:26\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "encoder = categorical_encoding(X,'school_state')\n",
    "\n",
    "X_train_school_state = encoder.transform(X_train['school_state'].values)\n",
    "X_test_school_state = encoder.transform(X_test['school_state'].values)\n",
    "\n",
    "embedding_school_state_len = len_embedding(X_train_school_state)\n",
    "\n",
    "print(\"Length of {} embedding is:{}\".format('school_state',embedding_school_state_len))\n",
    "\n",
    "embedding_school_state_unique_values = X_train['school_state'].nunique()\n",
    "print(embedding_school_state_unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of teacher_prefix embedding is:3\n",
      "5\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "encoder = categorical_encoding(X,'teacher_prefix')\n",
    "\n",
    "X_train_teacher_prefix = encoder.transform(X_train['teacher_prefix'].values)\n",
    "X_test_teacher_prefix = encoder.transform(X_test['teacher_prefix'].values)\n",
    "\n",
    "embedding_teacher_prefix_len = len_embedding(X_train_teacher_prefix)\n",
    "\n",
    "print(\"Length of {} embedding is:{}\".format('teacher_prefix',embedding_teacher_prefix_len))\n",
    "embedding_teacher_prefix_unique_value = X_train['teacher_prefix'].nunique()\n",
    "print(embedding_teacher_prefix_unique_value)\n",
    "print(type(X_train_teacher_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of clean_categories embedding is:25\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "encoder = categorical_encoding(X,'clean_categories')\n",
    "\n",
    "X_train_clean_categories = encoder.transform(X_train['clean_categories'].values)\n",
    "X_test_clean_categories = encoder.transform(X_test['clean_categories'].values)\n",
    "\n",
    "embedding_clean_categories_len = len_embedding(X_train_clean_categories)\n",
    "\n",
    "print(\"Length of {} embedding is:{}\".format('clean_categories',embedding_clean_categories_len))\n",
    "embedding_clean_categories_unique_value = X_train['clean_categories'].nunique()\n",
    "print(embedding_clean_categories_unique_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of clean_subcategories embedding is:50\n",
      "395\n"
     ]
    }
   ],
   "source": [
    "encoder = categorical_encoding(X,'clean_subcategories')\n",
    "\n",
    "X_train_clean_subcategories = encoder.transform(X_train['clean_subcategories'].values)\n",
    "X_test_clean_subcategories = encoder.transform(X_test['clean_subcategories'].values)\n",
    "\n",
    "embedding_clean_subcategories_len = len_embedding(X_train_clean_subcategories)\n",
    "\n",
    "print(\"Length of {} embedding is:{}\".format('clean_subcategories',embedding_clean_subcategories_len))\n",
    "embedding_clean_subcategories_unique_value = X_train['clean_subcategories'].nunique()\n",
    "print(embedding_clean_subcategories_unique_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of project_grade_category embedding is:2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "encoder = categorical_encoding(X,'project_grade_category')\n",
    "\n",
    "X_train_project_grade_category = encoder.transform(X_train['project_grade_category'].values)\n",
    "X_test_project_grade_category = encoder.transform(X_test['project_grade_category'].values)\n",
    "\n",
    "embedding_project_grade_category_len = len_embedding(X_train_project_grade_category)\n",
    "\n",
    "print(\"Length of {} embedding is:{}\".format('project_grade_category',embedding_project_grade_category_len))\n",
    "embedding_project_grade_category_unique_value = X_train['project_grade_category'].nunique()\n",
    "print(embedding_project_grade_category_unique_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features\n",
    "    1. teacher_number_of_previously_posted_projects\n",
    "    2. price\n",
    "####  Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    s = StandardScaler()\n",
    "    s.fit(X)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler expects 2d array\n",
    "X_train_teacher_number_of_previously_posted_projects = X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)\n",
    "\n",
    "s = normalization(X_train_teacher_number_of_previously_posted_projects)\n",
    "\n",
    "X_train_teacher_number_of_previously_posted_projects = s.transform(X_train_teacher_number_of_previously_posted_projects)\n",
    "\n",
    "X_test_teacher_number_of_previously_posted_projects = X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)\n",
    "X_test_teacher_number_of_previously_posted_projects = s.transform(X_test_teacher_number_of_previously_posted_projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler expects 2d array\n",
    "X_train_price = X_train['price'].values.reshape(-1,1)\n",
    "\n",
    "s = normalization(X_train_price)\n",
    "\n",
    "X_train_price = s.transform(X_train_price)\n",
    "\n",
    "X_test_price = X_test['price'].values.reshape(-1,1)\n",
    "X_test_price = s.transform(X_test_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_features = np.concatenate([X_train_teacher_number_of_previously_posted_projects,X_train_price],axis=1) \n",
    "X_test_num_features = np.concatenate([X_test_teacher_number_of_previously_posted_projects,X_test_price],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Multiple runnings of same model will be required \n",
    "\n",
    "\n",
    "# Input 1 , essay_input layer\n",
    "\n",
    "essay_input = Input(shape=(300,), name='essay_input')\n",
    "\n",
    "embedding_essay = Embedding(input_dim = max_words,output_dim = 300,\n",
    "                            input_length=300 , weights=[embeddings_mat])(essay_input)\n",
    "\n",
    "\n",
    "reg = l2(0.001)\n",
    "\n",
    "#100 cell LSTM unit\n",
    "lstm_essay = LSTM(100,recurrent_dropout=0.5,kernel_regularizer=reg,return_sequences=True)(embedding_essay)\n",
    "\n",
    "flatten_essay = Flatten()(lstm_essay)\n",
    "\n",
    "#https://stackoverflow.com/questions/66873192/subclassing-a-keras-layer-the-following-variables-were-used-a-lambda-layers-ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 2 , school_state feature\n",
    "\n",
    "school_state_input = Input(shape=(1,), name='school_state_input')\n",
    "\n",
    "embedding_school_state = Embedding(input_dim=embedding_school_state_unique_values+1,\n",
    "                                   output_dim=embedding_school_state_len , input_length=1)(school_state_input)\n",
    "\n",
    "flatten_school_state = Flatten()(embedding_school_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_prefix_input = Input(shape=(1,), name='teacher_prefix_input')\n",
    "\n",
    "embedding_teacher_prefix = Embedding(input_dim=embedding_teacher_prefix_unique_value+1,\n",
    "                                     output_dim=embedding_teacher_prefix_len ,input_length=1)(teacher_prefix_input)\n",
    "\n",
    "flatten_teacher_prefix = Flatten()(embedding_teacher_prefix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_categories_input = Input(shape=(1,), name='clean_categories_input')\n",
    "\n",
    "embedding_clean_categories = Embedding(input_dim=embedding_clean_categories_unique_value+1,\n",
    "                                       output_dim=embedding_clean_categories_len, input_length=1)(clean_categories_input)\n",
    "\n",
    "flatten_clean_categories = Flatten()(embedding_clean_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_subcategories_input = Input(shape=(1,), name='clean_subcategories_input')\n",
    "\n",
    "embedding_clean_subcategories = Embedding(input_dim=401,\n",
    "                                          output_dim=embedding_clean_subcategories_len, input_length=1)(clean_subcategories_input)\n",
    "\n",
    "flatten__clean_subcategories = Flatten()(embedding_clean_subcategories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_grade_category_input = Input(shape=(1,), name='project_grade_category_input')\n",
    "\n",
    "embedding_project_grade_category = Embedding(input_dim=embedding_project_grade_category_unique_value+1,\n",
    "                                     output_dim=embedding_project_grade_category_len,input_length=1)(project_grade_category_input)\n",
    "\n",
    "flatten_project_grade_category = Flatten()(embedding_project_grade_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_features_input = Input(shape=(2,) , name=\"numerical_features_input\")\n",
    "numeric_features_dense_layer = Dense(128, activation='relu' , kernel_initializer='he_normal',kernel_regularizer=l2(0.001))(numeric_features_input )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layers = [flatten_essay, flatten_school_state,flatten_teacher_prefix,flatten_clean_categories,\n",
    "                  flatten__clean_subcategories,flatten_project_grade_category,numeric_features_dense_layer]\n",
    "\n",
    "concat_layer = concatenate(flatten_layers)\n",
    "\n",
    "model = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(concat_layer)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(200,activation=\"relu\",kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(80,activation=\"relu\", kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output_layer')(model)\n",
    "\n",
    "\n",
    "model1 = Model(inputs=[essay_input, school_state_input ,teacher_prefix_input,clean_categories_input,\n",
    "                       clean_subcategories_input ,project_grade_category_input ,numeric_features_input],outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay_input (InputLayer)        [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 300)     15536100    essay_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "school_state_input (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix_input (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories_input (InputLa [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories_input (Inpu [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category_input (I [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 300, 100)     160400      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 26)        1352        school_state_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 3)         18          teacher_prefix_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 26)        1352        clean_categories_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 50)        20050       clean_subcategories_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 2)         10          project_grade_category_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "numerical_features_input (Input [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 30000)        0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 26)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 26)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 50)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          384         numerical_features_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30235)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          9070800     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          60200       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200)          800         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           16080       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            162         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,867,708\n",
      "Trainable params: 24,867,308\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "def custom_auroc(y_true, y_pred):\n",
    "    return tf.py_function(helper, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to define a custome function for calculating auroc\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[custom_auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final X_train and X_test \n",
    "X_Train_consolidated = [X_train_essay,X_train_school_state,X_train_teacher_prefix,\n",
    "                        X_train_clean_categories,X_train_clean_subcategories,\n",
    "                        X_train_project_grade_category,X_train_num_features]\n",
    "\n",
    "\n",
    "\n",
    "X_Test_consolidated = [X_test_essay,X_test_school_state,X_test_teacher_prefix,\n",
    "                       X_test_clean_categories,X_test_clean_subcategories,\n",
    "                       X_test_project_grade_category,X_test_num_features]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87398, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_essay.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't run again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 2)\n"
     ]
    }
   ],
   "source": [
    "#https://keras.io/api/utils/python_utils/#to_categorical-function\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 2) \n",
    "y_test = to_categorical(y_test, 2)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.gitmemory.com/issue/tensorflow/tensorflow/28799/532818343\n",
    "\n",
    "\n",
    "checkpoint_object = ModelCheckpoint('model1.h5',monitor=\"val_custom_auroc\",mode=\"max\",save_best_only = True,verbose=2)\n",
    "\n",
    "\n",
    "earlystop_object = EarlyStopping(monitor = 'val_custom_auroc', mode=\"max\",patience = 4,verbose = 2)\n",
    "\n",
    "logdir = os.path.join(\"logs\", 'model1_visualization')\n",
    "\n",
    "tensorboard_object = TensorBoard(log_dir=logdir,histogram_freq=1)\n",
    "\n",
    "callbacks = [checkpoint_object,earlystop_object, tensorboard_object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 1.2881 - custom_auroc: 0.5843\n",
      "Epoch 00001: val_custom_auroc improved from -inf to 0.72132, saving model to model1.h5\n",
      "87398/87398 [==============================] - 326s 4ms/sample - loss: 1.2875 - custom_auroc: 0.5845 - val_loss: 0.9043 - val_custom_auroc: 0.7213\n",
      "Epoch 2/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.7137 - custom_auroc: 0.7178\n",
      "Epoch 00002: val_custom_auroc improved from 0.72132 to 0.74838, saving model to model1.h5\n",
      "87398/87398 [==============================] - 334s 4ms/sample - loss: 0.7136 - custom_auroc: 0.7180 - val_loss: 0.6369 - val_custom_auroc: 0.7484\n",
      "Epoch 3/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.5644 - custom_auroc: 0.7618\n",
      "Epoch 00003: val_custom_auroc improved from 0.74838 to 0.75185, saving model to model1.h5\n",
      "87398/87398 [==============================] - 336s 4ms/sample - loss: 0.5642 - custom_auroc: 0.7622 - val_loss: 0.6074 - val_custom_auroc: 0.7518\n",
      "Epoch 4/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4743 - custom_auroc: 0.7960\n",
      "Epoch 00004: val_custom_auroc did not improve from 0.75185\n",
      "87398/87398 [==============================] - 314s 4ms/sample - loss: 0.4743 - custom_auroc: 0.7960 - val_loss: 0.5153 - val_custom_auroc: 0.7474\n",
      "Epoch 5/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4052 - custom_auroc: 0.8379\n",
      "Epoch 00005: val_custom_auroc did not improve from 0.75185\n",
      "87398/87398 [==============================] - 312s 4ms/sample - loss: 0.4053 - custom_auroc: 0.8376 - val_loss: 0.5365 - val_custom_auroc: 0.7299\n",
      "Epoch 6/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.3451 - custom_auroc: 0.8794\n",
      "Epoch 00006: val_custom_auroc did not improve from 0.75185\n",
      "87398/87398 [==============================] - 313s 4ms/sample - loss: 0.3451 - custom_auroc: 0.8793 - val_loss: 0.4869 - val_custom_auroc: 0.7124\n",
      "Epoch 7/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.2984 - custom_auroc: 0.9096\n",
      "Epoch 00007: val_custom_auroc did not improve from 0.75185\n",
      "87398/87398 [==============================] - 315s 4ms/sample - loss: 0.2985 - custom_auroc: 0.9092 - val_loss: 0.5114 - val_custom_auroc: 0.6797\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_Train_consolidated, y_train, \n",
    "                     batch_size=256, epochs=10, verbose=1,\n",
    "                     callbacks=callbacks, validation_data=(X_Test_consolidated,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75184745\n"
     ]
    }
   ],
   "source": [
    "Ass_1_model1 = np.max(model1.history.history['val_custom_auroc'])\n",
    "print(Ass_1_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3ed208e9e682e4c9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3ed208e9e682e4c9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_2 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.0001))(concat_layer)\n",
    "model1_2 = Dropout(0.2)(model1_2)\n",
    "model1_2 = Dense(200,activation=\"relu\",kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.0001))(model1_2)\n",
    "model1_2 = BatchNormalization()(model1_2)\n",
    "model1_2 = Dropout(0.5)(model1_2)\n",
    "model1_2 = Dense(80,activation=\"relu\", kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.0001))(model1_2)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output_layer')(model1_2)\n",
    "\n",
    "\n",
    "model1_2 = Model(inputs=[essay_input, school_state_input ,teacher_prefix_input,clean_categories_input,\n",
    "                       clean_subcategories_input ,project_grade_category_input ,numeric_features_input],outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[custom_auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_object = ModelCheckpoint('model2.h5',monitor=\"val_custom_auroc\",mode=\"max\",save_best_only = True,verbose=2)\n",
    "\n",
    "\n",
    "earlystop_object = EarlyStopping(monitor = 'val_custom_auroc', mode=\"max\",patience = 4,verbose = 2)\n",
    "\n",
    "logdir = os.path.join(\"logs\", 'model1_2_visualization')\n",
    "\n",
    "tensorboard_object = TensorBoard(log_dir=logdir,histogram_freq=1)\n",
    "\n",
    "callbacks = [checkpoint_object,earlystop_object, tensorboard_object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_custom_auroc improved from -inf to 0.63384, saving model to model2.h5\n",
      "87398/87398 - 265s - loss: 0.3127 - custom_auroc: 0.9297 - val_loss: 0.6732 - val_custom_auroc: 0.6338\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_custom_auroc improved from 0.63384 to 0.64420, saving model to model2.h5\n",
      "87398/87398 - 265s - loss: 0.2738 - custom_auroc: 0.9402 - val_loss: 0.6685 - val_custom_auroc: 0.6442\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_custom_auroc improved from 0.64420 to 0.65200, saving model to model2.h5\n",
      "87398/87398 - 265s - loss: 0.2493 - custom_auroc: 0.9472 - val_loss: 0.6524 - val_custom_auroc: 0.6520\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_custom_auroc did not improve from 0.65200\n",
      "87398/87398 - 262s - loss: 0.2315 - custom_auroc: 0.9534 - val_loss: 0.6737 - val_custom_auroc: 0.6497\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_custom_auroc did not improve from 0.65200\n",
      "87398/87398 - 273s - loss: 0.2175 - custom_auroc: 0.9594 - val_loss: 0.7265 - val_custom_auroc: 0.6371\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_custom_auroc did not improve from 0.65200\n",
      "87398/87398 - 279s - loss: 0.2035 - custom_auroc: 0.9651 - val_loss: 0.6895 - val_custom_auroc: 0.6263\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_custom_auroc did not improve from 0.65200\n",
      "87398/87398 - 280s - loss: 0.1888 - custom_auroc: 0.9725 - val_loss: 0.7767 - val_custom_auroc: 0.6350\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model1_2.fit(X_Train_consolidated, y_train, \n",
    "                     batch_size = 512, epochs=10, verbose=2,\n",
    "                     callbacks=callbacks, validation_data=(X_Test_consolidated,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-63649067f1f3cf67\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-63649067f1f3cf67\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6520036\n"
     ]
    }
   ],
   "source": [
    "Ass_1_model2 = np.max(model1_2.history.history['val_custom_auroc'])\n",
    "print(Ass_1_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to minimize loss rather than maximizing area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 20.07366, saving model to model1_3.h5\n",
      "87398/87398 - 279s - loss: 5.0974 - custom_auroc: 0.5400 - val_loss: 20.0737 - val_custom_auroc: 0.5763\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 20.07366\n",
      "87398/87398 - 270s - loss: 27.1702 - custom_auroc: 0.5348 - val_loss: 29.8800 - val_custom_auroc: 0.5657\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 20.07366\n",
      "87398/87398 - 279s - loss: 25.9079 - custom_auroc: 0.5303 - val_loss: 31.1491 - val_custom_auroc: 0.5610\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 20.07366\n",
      "87398/87398 - 288s - loss: 40.7626 - custom_auroc: 0.5348 - val_loss: 40.7995 - val_custom_auroc: 0.5664\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 20.07366\n",
      "87398/87398 - 273s - loss: 50.4070 - custom_auroc: 0.5307 - val_loss: 63.6516 - val_custom_auroc: 0.5584\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "flatten_layers = [flatten_essay, flatten_school_state,flatten_teacher_prefix,flatten_clean_categories,\n",
    "                  flatten__clean_subcategories,flatten_project_grade_category,numeric_features_dense_layer]\n",
    "\n",
    "concat_layer = concatenate(flatten_layers)\n",
    "\n",
    "\n",
    "model1_3 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(concat_layer)\n",
    "model1_3 = Dropout(0.2)(model1_3)\n",
    "model1_3 = Dense(200,activation=\"relu\",kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model1_3)\n",
    "model1_3 = BatchNormalization()(model1_3)\n",
    "model1_3 = Dropout(0.5)(model1_3)\n",
    "model1_3 = Dense(80,activation=\"sigmoid\", kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model1_3)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output_layer')(model1_3)\n",
    "\n",
    "\n",
    "model1_3 = Model(inputs=[essay_input, school_state_input ,teacher_prefix_input,clean_categories_input,\n",
    "                       clean_subcategories_input ,project_grade_category_input ,numeric_features_input],outputs=[output])\n",
    "\n",
    "\n",
    "model1_3.compile(optimizer=Adam(0.05), loss='categorical_crossentropy', metrics=[custom_auroc])\n",
    "\n",
    "checkpoint_object = ModelCheckpoint('model1_3.h5',monitor=\"val_loss\",mode=\"min\",save_best_only = True,verbose=2)\n",
    "\n",
    "\n",
    "earlystop_object = EarlyStopping(monitor = 'val_loss', mode=\"min\",patience = 4,verbose = 2)\n",
    "\n",
    "logdir = os.path.join(\"logs\", 'model1_3_visualization')\n",
    "tensorboard_object = TensorBoard(log_dir=logdir,histogram_freq=1)\n",
    "\n",
    "callbacks = [checkpoint_object,earlystop_object, tensorboard_object]\n",
    "\n",
    "\n",
    "history = model1_3.fit(X_Train_consolidated, y_train, \n",
    "                     batch_size = 512, epochs=10, verbose=2,\n",
    "                     callbacks=callbacks, validation_data=(X_Test_consolidated,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57626003\n"
     ]
    }
   ],
   "source": [
    "Ass_1_model3 = np.max(model1_3.history.history['val_custom_auroc'])\n",
    "print(Ass_1_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e6ca63b4ecb837af\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e6ca63b4ecb837af\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Train the TF-IDF on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. <br>\n",
    "3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)<br>\n",
    "4. Train the LSTM after removing the Low and High idf value words. (In model-1 Train on total data but in Model-2 train on data after removing some words based on IDF values)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(min_df=10,max_features=10000) #Defining TFIDF with min_df=10\n",
    "tf_idf_val = tf_idf_vect.fit(X_train['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract idf values\n",
    "idfs = tf_idf_val.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dataframe for idfs\n",
    "df_idf = pd.DataFrame(idfs, columns=['idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.190853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.942089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.482059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.816073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.120744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idf\n",
       "0  7.190853\n",
       "1  5.942089\n",
       "2  4.482059\n",
       "3  3.816073\n",
       "4  7.120744"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf.to_csv('df_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0 percentile is: 1.0074995326946303 \n",
      "Top 10 percentile is: 4.961139378414857 \n",
      "Top 20 percentile is: 5.87995697045414 \n",
      "Top 30 percentile is: 6.590422901704701 \n",
      "Top 40 percentile is: 7.115548931025689 \n",
      "Top 50 percentile is: 7.549925382628272 \n",
      "Top 60 percentile is: 7.900902305452367 \n",
      "Top 70 percentile is: 8.203851850034937 \n",
      "Top 80 percentile is: 8.507038109022682 \n",
      "Top 90 percentile is: 8.794720181474464 \n",
      "Top 100 percentile is: 9.893332470142573 \n"
     ]
    }
   ],
   "source": [
    "#See the distribution of idf values\n",
    "percentiles = np.percentile(df_idf['idf'],[i for i in range(0,110,10)])\n",
    "p= 0\n",
    "for i in percentiles:\n",
    "    print('Top {} percentile is: {} '.format(p,i))\n",
    "    p += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Boxplot can also give a nice insight into distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data=df_idf,y='idf')\n",
    "plt.plot()\n",
    "plt.title('Distribution of IDF values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can consider all idf values lying bw. 25th to 75th percentiles\n",
    "p_start = np.percentile(idfs,[25])\n",
    "p_end = np.percentile(idfs,[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.893332470142573 1.0074995326946303\n"
     ]
    }
   ],
   "source": [
    "print(np.max(idfs),np.min(idfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking words lying in interquartile range only\n",
    "def iqr_words(tf_idf_val,p_start,p_end):\n",
    "    zipped = zip(tf_idf_val.get_feature_names(),idfs)\n",
    "    \n",
    "    words = []\n",
    "    for word , idf in zipped:\n",
    "        if idf >= p_start and idf <= p_end:\n",
    "            words.append(word)\n",
    "    \n",
    "    return np.asarray(words)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_words = iqr_words(tf_idf_val,p_start,p_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5030\n"
     ]
    }
   ],
   "source": [
    "print(len(imp_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5030,)\n"
     ]
    }
   ],
   "source": [
    "print(imp_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_start = 2\n",
    "p_end = 11\n",
    "imp_words_2_11 = iqr_words(tf_idf_val,p_start,p_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9975,)\n"
     ]
    }
   ],
   "source": [
    "print(imp_words_2_11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make embeddings from tokenizer on essay using imp words only\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_in_iqr(essay_text,imp_words):\n",
    "    '''\n",
    "    This method collects words within 25 to 75 percentile\n",
    "    '''\n",
    "    result = []\n",
    "    # For each sentence , keep only words those are in impwords\n",
    "    for sent in tqdm(essay_text):\n",
    "        words = sent.split()\n",
    "        final_sent = ''\n",
    "\n",
    "        for word in words:\n",
    "            # Check word exists in idf corpus\n",
    "            if(word in imp_words):\n",
    "                final_sent += ' ' + word\n",
    "\n",
    "        result.append(final_sent)\n",
    "\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 87398/87398 [09:34<00:00, 152.02it/s]\n"
     ]
    }
   ],
   "source": [
    "idf_essay_text = text_in_iqr(X_train['essay'],imp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 21850/21850 [02:23<00:00, 152.74it/s]\n"
     ]
    }
   ],
   "source": [
    "idf_essay_text_test = text_in_iqr(X_test['essay'],imp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " virginia beach virginia cluster heights heights obvious nurture storyworks storyworks outcomes\n"
     ]
    }
   ],
   "source": [
    "print(idf_essay_text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87398\n",
      "<class 'numpy.ndarray'>\n",
      " accommodations ict dyslexia bingo shift puzzle caddies crafts convenient\n"
     ]
    }
   ],
   "source": [
    "print(len(idf_essay_text))\n",
    "print(type(idf_essay_text))\n",
    "print(idf_essay_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 87398/87398 [17:19<00:00, 84.05it/s]\n"
     ]
    }
   ],
   "source": [
    "idf_essay_text_2_11 = text_in_iqr(X_train['essay'],imp_words_2_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 21850/21850 [04:11<00:00, 86.87it/s] \n"
     ]
    }
   ],
   "source": [
    "idf_essay_text_test_2_11 = text_in_iqr(X_test['essay'],imp_words_2_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(idf_essay_text)\n",
    "\n",
    "vocab_size = len(t.word_index)+1\n",
    "embedding_mat_idf = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_mat_idf[i] = embedding_vector\n",
    "\n",
    "idf_essay_text = t.texts_to_sequences(idf_essay_text)\n",
    "idf_essay_text_test = t.texts_to_sequences(idf_essay_text_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_essay_idf = pad_sequences(idf_essay_text, maxlen=300)\n",
    "X_test_essay_idf  = pad_sequences(idf_essay_text_test, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_essay_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "\n",
    "#print(type(t.word_index)) - dictionary\n",
    "with open('glove_vectors','rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(idf_essay_text_2_11)\n",
    "\n",
    "vocab_size = len(t.word_index)+1\n",
    "embedding_mat_idf = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_mat_idf[i] = embedding_vector\n",
    "\n",
    "idf_essay_text_2_11 = t.texts_to_sequences(idf_essay_text_2_11)\n",
    "idf_essay_text_test_2_11 = t.texts_to_sequences(idf_essay_text_test_2_11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_essay_idf_2_11 = pad_sequences(idf_essay_text_2_11, maxlen=300)\n",
    "X_test_essay_idf_2_11  = pad_sequences(idf_essay_text_test_2_11, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final X_train and X_test \n",
    "X_Train_consolidated_2_11 = [X_train_essay_idf_2_11,X_train_school_state,X_train_teacher_prefix,\n",
    "                        X_train_clean_categories,X_train_clean_subcategories,\n",
    "                        X_train_project_grade_category,X_train_num_features]\n",
    "\n",
    "\n",
    "\n",
    "X_Test_consolidated_2_11 = [X_test_essay_idf_2_11,X_test_school_state,X_test_teacher_prefix,\n",
    "                       X_test_clean_categories,X_test_clean_subcategories,\n",
    "                       X_test_project_grade_category,X_test_num_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final X_train and X_test \n",
    "X_Train_consolidated = [X_train_essay_idf,X_train_school_state,X_train_teacher_prefix,\n",
    "                        X_train_clean_categories,X_train_clean_subcategories,\n",
    "                        X_train_project_grade_category,X_train_num_features]\n",
    "\n",
    "\n",
    "\n",
    "X_Test_consolidated = [X_test_essay_idf,X_test_school_state,X_test_teacher_prefix,\n",
    "                       X_test_clean_categories,X_test_clean_subcategories,\n",
    "                       X_test_project_grade_category,X_test_num_features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, 2) \n",
    "y_test = to_categorical(y_test, 2)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay_input (InputLayer)        [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 300)     2992800     essay_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "school_state_input (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix_input (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories_input (InputLa [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories_input (Inpu [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category_input (I [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 300, 100)     160400      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 26)        1352        school_state_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 3)         18          teacher_prefix_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 25)        1300        clean_categories_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 50)        20050       clean_subcategories_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 2)         10          project_grade_category_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "numerical_features_input (Input [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 30000)        0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 26)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 50)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          384         numerical_features_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30234)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          9070500     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          60200       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200)          800         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           16080       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            162         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,324,056\n",
      "Trainable params: 12,323,656\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Input 1 , essay_input layer\n",
    "\n",
    "essay_input = Input(shape=(300,), name='essay_input')\n",
    "embedding_essay = Embedding(input_dim = vocab_size,output_dim = 300,\n",
    "                            input_length=300 , weights=[embedding_mat_idf])(essay_input)\n",
    "reg = l2(0.001)\n",
    "#100 cell LSTM unit\n",
    "lstm_essay = LSTM(100,recurrent_dropout=0.5,kernel_regularizer=reg,return_sequences=True)(embedding_essay)\n",
    "flatten_essay = Flatten()(lstm_essay)\n",
    "\n",
    "# Input 2 , school_state feature\n",
    "\n",
    "school_state_input = Input(shape=(1,), name='school_state_input')\n",
    "embedding_school_state = Embedding(input_dim=embedding_school_state_unique_values+1,\n",
    "                                   output_dim=embedding_school_state_len , input_length=1)(school_state_input)\n",
    "\n",
    "flatten_school_state = Flatten()(embedding_school_state)\n",
    "\n",
    "# Input 3 , school_state feature\n",
    "teacher_prefix_input = Input(shape=(1,), name='teacher_prefix_input')\n",
    "embedding_teacher_prefix = Embedding(input_dim=embedding_teacher_prefix_unique_value+1,\n",
    "                                     output_dim=embedding_teacher_prefix_len ,input_length=1)(teacher_prefix_input)\n",
    "\n",
    "flatten_teacher_prefix = Flatten()(embedding_teacher_prefix)\n",
    "\n",
    "# Input 4 , clean_categories feature\n",
    "clean_categories_input = Input(shape=(1,), name='clean_categories_input')\n",
    "embedding_clean_categories = Embedding(input_dim=embedding_clean_categories_unique_value+2,\n",
    "                                       output_dim=embedding_clean_categories_len, input_length=1)(clean_categories_input)\n",
    "\n",
    "flatten_clean_categories = Flatten()(embedding_clean_categories)\n",
    "\n",
    "# Input 5 , clean_subcategories feature\n",
    "clean_subcategories_input = Input(shape=(1,), name='clean_subcategories_input')\n",
    "embedding_clean_subcategories = Embedding(input_dim=401,\n",
    "                                          output_dim=embedding_clean_subcategories_len, input_length=1)(clean_subcategories_input)\n",
    "\n",
    "flatten__clean_subcategories = Flatten()(embedding_clean_subcategories)\n",
    "\n",
    "# Input 6 , project_grade_category feature\n",
    "project_grade_category_input = Input(shape=(1,), name='project_grade_category_input')\n",
    "embedding_project_grade_category = Embedding(input_dim=embedding_project_grade_category_unique_value+1,\n",
    "                                     output_dim=embedding_project_grade_category_len,input_length=1)(project_grade_category_input)\n",
    "\n",
    "flatten_project_grade_category = Flatten()(embedding_project_grade_category)\n",
    "\n",
    "# Input 7 , Numerical feature\n",
    "numeric_features_input = Input(shape=(2,) , name=\"numerical_features_input\")\n",
    "numeric_features_dense_layer = Dense(128, activation='relu' , kernel_initializer='he_normal',kernel_regularizer=l2(0.001))(numeric_features_input )\n",
    "\n",
    "\n",
    "\n",
    "flatten_layers = [flatten_essay, flatten_school_state,flatten_teacher_prefix,flatten_clean_categories,\n",
    "                  flatten__clean_subcategories,flatten_project_grade_category,numeric_features_dense_layer]\n",
    "\n",
    "concat_layer = concatenate(flatten_layers)\n",
    "\n",
    "model2_1 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(concat_layer)\n",
    "model2_1 = Dropout(0.5)(model2_1)\n",
    "model2_1 = Dense(200,activation=\"relu\",kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model2_1)\n",
    "model2_1 = BatchNormalization()(model2_1)\n",
    "model2_1 = Dropout(0.5)(model2_1)\n",
    "model2_1 = Dense(80,activation=\"relu\", kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model2_1)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output_layer')(model2_1)\n",
    "\n",
    "\n",
    "model2_1 = Model(inputs=[essay_input, school_state_input ,teacher_prefix_input,clean_categories_input,\n",
    "                       clean_subcategories_input ,project_grade_category_input ,numeric_features_input],outputs=[output])\n",
    "\n",
    "\n",
    "print(model2_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "def custom_auroc(y_true, y_pred):\n",
    "    return tf.py_function(helper, (y_true, y_pred), tf.double)\n",
    "    \n",
    "#need to define a custome function for calculating auroc\n",
    "\n",
    "model2_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[custom_auroc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.gitmemory.com/issue/tensorflow/tensorflow/28799/532818343\n",
    "\n",
    "\n",
    "checkpoint_object = ModelCheckpoint('Ass2_Model1.h5',monitor=\"val_custom_auroc\",mode=\"max\",save_best_only = True,verbose=2)\n",
    "\n",
    "\n",
    "earlystop_object = EarlyStopping(monitor = 'val_custom_auroc', mode=\"max\",patience = 2,verbose = 2)\n",
    "\n",
    "logdir = os.path.join(\"logs\",'Ass2_Model1_visualization')\n",
    "\n",
    "tensorboard_object = TensorBoard(log_dir=logdir,histogram_freq=1)\n",
    "\n",
    "callbacks = [checkpoint_object,earlystop_object, tensorboard_object]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 1.1354 - custom_auroc: 0.5536\n",
      "Epoch 00001: val_custom_auroc improved from -inf to 0.61264, saving model to Ass2_Model1.h5\n",
      "87398/87398 [==============================] - 290s 3ms/sample - loss: 1.1351 - custom_auroc: 0.5536 - val_loss: 0.8161 - val_custom_auroc: 0.6126\n",
      "Epoch 2/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.7184 - custom_auroc: 0.6179\n",
      "Epoch 00002: val_custom_auroc improved from 0.61264 to 0.65989, saving model to Ass2_Model1.h5\n",
      "87398/87398 [==============================] - 301s 3ms/sample - loss: 0.7182 - custom_auroc: 0.6183 - val_loss: 0.6275 - val_custom_auroc: 0.6599\n",
      "Epoch 3/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.5817 - custom_auroc: 0.6644\n",
      "Epoch 00003: val_custom_auroc improved from 0.65989 to 0.66057, saving model to Ass2_Model1.h5\n",
      "87398/87398 [==============================] - 285s 3ms/sample - loss: 0.5818 - custom_auroc: 0.6643 - val_loss: 0.5383 - val_custom_auroc: 0.6606\n",
      "Epoch 4/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.5085 - custom_auroc: 0.6925\n",
      "Epoch 00004: val_custom_auroc improved from 0.66057 to 0.66531, saving model to Ass2_Model1.h5\n",
      "87398/87398 [==============================] - 283s 3ms/sample - loss: 0.5085 - custom_auroc: 0.6925 - val_loss: 0.4998 - val_custom_auroc: 0.6653\n",
      "Epoch 5/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4622 - custom_auroc: 0.7087\n",
      "Epoch 00005: val_custom_auroc did not improve from 0.66531\n",
      "87398/87398 [==============================] - 283s 3ms/sample - loss: 0.4622 - custom_auroc: 0.7087 - val_loss: 0.4629 - val_custom_auroc: 0.6587\n",
      "Epoch 6/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4340 - custom_auroc: 0.7242\n",
      "Epoch 00006: val_custom_auroc did not improve from 0.66531\n",
      "87398/87398 [==============================] - 284s 3ms/sample - loss: 0.4340 - custom_auroc: 0.7244 - val_loss: 0.4478 - val_custom_auroc: 0.6600\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model2_1.fit(X_Train_consolidated, y_train, \n",
    "                     batch_size=256, epochs=10, verbose=1,\n",
    "                     callbacks=callbacks, validation_data=(X_Test_consolidated,y_test))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6653102\n"
     ]
    }
   ],
   "source": [
    "Ass_2_model1 = np.max(model2_1.history.history['val_custom_auroc'])\n",
    "print(Ass_2_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-89b6a3fa2a725d75\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-89b6a3fa2a725d75\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2_2 with words idf value in range 2 to 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay_input (InputLayer)        [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 300, 300)     2992800     essay_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "school_state_input (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix_input (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_categories_input (InputLa [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "clean_subcategories_input (Inpu [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category_input (I [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300, 100)     160400      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 26)        1352        school_state_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 3)         18          teacher_prefix_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 25)        1300        clean_categories_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 50)        20050       clean_subcategories_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 2)         10          project_grade_category_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "numerical_features_input (Input [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 30000)        0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 26)           0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 3)            0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 25)           0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 50)           0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 2)            0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          384         numerical_features_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30234)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 300)          9070500     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 300)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 200)          60200       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 80)           16080       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            162         dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,324,056\n",
      "Trainable params: 12,323,656\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Input 1 , essay_input layer\n",
    "\n",
    "essay_input = Input(shape=(300,), name='essay_input')\n",
    "embedding_essay = Embedding(input_dim = vocab_size,output_dim = 300,\n",
    "                            input_length=300 , weights=[embedding_mat_idf])(essay_input)\n",
    "reg = l2(0.001)\n",
    "#100 cell LSTM unit\n",
    "lstm_essay = LSTM(100,recurrent_dropout=0.5,kernel_regularizer=reg,return_sequences=True)(embedding_essay)\n",
    "flatten_essay = Flatten()(lstm_essay)\n",
    "\n",
    "# Input 2 , school_state feature\n",
    "\n",
    "school_state_input = Input(shape=(1,), name='school_state_input')\n",
    "embedding_school_state = Embedding(input_dim=embedding_school_state_unique_values+1,\n",
    "                                   output_dim=embedding_school_state_len , input_length=1)(school_state_input)\n",
    "\n",
    "flatten_school_state = Flatten()(embedding_school_state)\n",
    "\n",
    "# Input 3 , school_state feature\n",
    "teacher_prefix_input = Input(shape=(1,), name='teacher_prefix_input')\n",
    "embedding_teacher_prefix = Embedding(input_dim=embedding_teacher_prefix_unique_value+1,\n",
    "                                     output_dim=embedding_teacher_prefix_len ,input_length=1)(teacher_prefix_input)\n",
    "\n",
    "flatten_teacher_prefix = Flatten()(embedding_teacher_prefix)\n",
    "\n",
    "# Input 4 , clean_categories feature\n",
    "clean_categories_input = Input(shape=(1,), name='clean_categories_input')\n",
    "embedding_clean_categories = Embedding(input_dim=embedding_clean_categories_unique_value+2,\n",
    "                                       output_dim=embedding_clean_categories_len, input_length=1)(clean_categories_input)\n",
    "\n",
    "flatten_clean_categories = Flatten()(embedding_clean_categories)\n",
    "\n",
    "# Input 5 , clean_subcategories feature\n",
    "clean_subcategories_input = Input(shape=(1,), name='clean_subcategories_input')\n",
    "embedding_clean_subcategories = Embedding(input_dim=401,\n",
    "                                          output_dim=embedding_clean_subcategories_len, input_length=1)(clean_subcategories_input)\n",
    "\n",
    "flatten__clean_subcategories = Flatten()(embedding_clean_subcategories)\n",
    "\n",
    "# Input 6 , project_grade_category feature\n",
    "project_grade_category_input = Input(shape=(1,), name='project_grade_category_input')\n",
    "embedding_project_grade_category = Embedding(input_dim=embedding_project_grade_category_unique_value+1,\n",
    "                                     output_dim=embedding_project_grade_category_len,input_length=1)(project_grade_category_input)\n",
    "\n",
    "flatten_project_grade_category = Flatten()(embedding_project_grade_category)\n",
    "\n",
    "# Input 7 , Numerical feature\n",
    "numeric_features_input = Input(shape=(2,) , name=\"numerical_features_input\")\n",
    "numeric_features_dense_layer = Dense(128, activation='relu' , kernel_initializer='he_normal',kernel_regularizer=l2(0.001))(numeric_features_input )\n",
    "\n",
    "\n",
    "\n",
    "flatten_layers = [flatten_essay, flatten_school_state,flatten_teacher_prefix,flatten_clean_categories,\n",
    "                  flatten__clean_subcategories,flatten_project_grade_category,numeric_features_dense_layer]\n",
    "\n",
    "concat_layer = concatenate(flatten_layers)\n",
    "\n",
    "model2_2 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(concat_layer)\n",
    "model2_2 = Dropout(0.5)(model2_2)\n",
    "model2_2 = Dense(200,activation=\"relu\",kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model2_2)\n",
    "model2_2 = BatchNormalization()(model2_2)\n",
    "model2_2 = Dropout(0.5)(model2_2)\n",
    "model2_2 = Dense(80,activation=\"relu\", kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model2_2)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output_layer')(model2_2)\n",
    "\n",
    "\n",
    "model2_2 = Model(inputs=[essay_input, school_state_input ,teacher_prefix_input,clean_categories_input,\n",
    "                       clean_subcategories_input ,project_grade_category_input ,numeric_features_input],outputs=[output])\n",
    "\n",
    "\n",
    "print(model2_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_object = ModelCheckpoint('Ass2_Model2.h5',monitor=\"val_custom_auroc\",mode=\"max\",save_best_only = True,verbose=2)\n",
    "\n",
    "\n",
    "earlystop_object = EarlyStopping(monitor = 'val_custom_auroc', mode=\"max\",patience = 2,verbose = 2)\n",
    "\n",
    "logdir = os.path.join(\"logs\",'Ass2_Model2_visualization')\n",
    "tensorboard_object = TensorBoard(log_dir=logdir,histogram_freq=1)\n",
    "\n",
    "callbacks = [checkpoint_object,earlystop_object, tensorboard_object]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.9993 - custom_auroc: 0.6157\n",
      "Epoch 00001: val_custom_auroc improved from -inf to 0.73834, saving model to Ass2_Model2.h5\n",
      "87398/87398 [==============================] - 291s 3ms/sample - loss: 0.9990 - custom_auroc: 0.6158 - val_loss: 0.7420 - val_custom_auroc: 0.7383\n",
      "Epoch 2/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.6066 - custom_auroc: 0.7251\n",
      "Epoch 00002: val_custom_auroc improved from 0.73834 to 0.74967, saving model to Ass2_Model2.h5\n",
      "87398/87398 [==============================] - 293s 3ms/sample - loss: 0.6064 - custom_auroc: 0.7256 - val_loss: 0.5219 - val_custom_auroc: 0.7497\n",
      "Epoch 3/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4879 - custom_auroc: 0.7550\n",
      "Epoch 00003: val_custom_auroc improved from 0.74967 to 0.75533, saving model to Ass2_Model2.h5\n",
      "87398/87398 [==============================] - 296s 3ms/sample - loss: 0.4879 - custom_auroc: 0.7551 - val_loss: 0.5244 - val_custom_auroc: 0.7553\n",
      "Epoch 4/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4312 - custom_auroc: 0.7701\n",
      "Epoch 00004: val_custom_auroc improved from 0.75533 to 0.75920, saving model to Ass2_Model2.h5\n",
      "87398/87398 [==============================] - 299s 3ms/sample - loss: 0.4311 - custom_auroc: 0.7702 - val_loss: 0.4241 - val_custom_auroc: 0.7592\n",
      "Epoch 5/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4010 - custom_auroc: 0.7801\n",
      "Epoch 00005: val_custom_auroc did not improve from 0.75920\n",
      "87398/87398 [==============================] - 297s 3ms/sample - loss: 0.4009 - custom_auroc: 0.7803 - val_loss: 0.4559 - val_custom_auroc: 0.7529\n",
      "Epoch 6/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.3831 - custom_auroc: 0.7897\n",
      "Epoch 00006: val_custom_auroc did not improve from 0.75920\n",
      "87398/87398 [==============================] - 294s 3ms/sample - loss: 0.3831 - custom_auroc: 0.7896 - val_loss: 0.4125 - val_custom_auroc: 0.7500\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "def helper(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "def custom_auroc(y_true, y_pred):\n",
    "    return tf.py_function(helper, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "model2_2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[custom_auroc])\n",
    "\n",
    "history = model2_2.fit(X_Train_consolidated_2_11, y_train, \n",
    "                     batch_size=256, epochs=10, verbose=1,\n",
    "                     callbacks=callbacks, validation_data=(X_Test_consolidated_2_11,y_test))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b4716687697df63e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b4716687697df63e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7592025\n"
     ]
    }
   ],
   "source": [
    "Ass_2_model2 = np.max(model2_2.history.history['val_custom_auroc'])\n",
    "print(Ass_2_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
    "ref: https://i.imgur.com/fkQ8nGo.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- __input_seq_total_text_data__: <br>\n",
    "<pre>\n",
    "    . Use text column('essay'), and use the Embedding layer to get word vectors. <br>\n",
    "    . Use given predefined glove word vectors, don't train any word vectors. <br>\n",
    "    . Use LSTM that is given above, get the LSTM output and Flatten that output. <br>\n",
    "    . You are free to preprocess the input text as you needed. <br>\n",
    "</pre>\n",
    "- __Other_than_text_data__:<br>\n",
    "<pre>\n",
    "    . Convert all your Categorical values to onehot coded and then concatenate all these onehot vectors <br>\n",
    "    . Neumerical values and use <a href='https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions'>CNN1D</a> as shown in above figure. <br>\n",
    "    . You are free to choose all CNN parameters like kernel sizes, stride.<br>\n",
    "    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chitresh/miniconda3/envs/new_env/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/chitresh/miniconda3/envs/new_env/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "Y = data['project_is_approved'].values\n",
    "X = data.drop(['project_is_approved'],axis = 1)\n",
    "\n",
    "\n",
    "# 80-20 split of data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,stratify=Y)\n",
    "\n",
    "\n",
    "len_embeddings = 300\n",
    "with open('glove_vectors','rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(X_train['essay'])\n",
    "\n",
    "#Convert text to sequencese\n",
    "X_train['essay_tokens'] = t.texts_to_sequences(np.asarray(X_train['essay']))\n",
    "X_test['essay_tokens'] = t.texts_to_sequences(np.asarray(X_test['essay']))\n",
    "X_train_essay = pad_sequences(X_train['essay_tokens'].values, maxlen=len_embeddings)\n",
    "X_test_essay  = pad_sequences(X_test['essay_tokens'].values, maxlen=len_embeddings)\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 2) \n",
    "y_test = to_categorical(y_test, 2)\n",
    "print(y_train.shape)\n",
    "\n",
    "max_words = len(t.word_index)+1\n",
    "embeddings_mat = np.zeros((max_words,300))\n",
    "\n",
    "#Keep the embeddings for words in our train data\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_mat[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51561\n"
     ]
    }
   ],
   "source": [
    "max_words = len(embeddings_mat)\n",
    "print(max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf\n",
    "def one_hot_encoding(X,feature):\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(np.asarray(X[feature]).reshape(-1,1))\n",
    "    return encoder\n",
    "\n",
    "encoder = one_hot_encoding(X_train,'school_state')\n",
    "\n",
    "X_train_school_state_one = encoder.transform(np.asarray(X_train['school_state']).reshape(-1,1))\n",
    "X_test_school_state_one = encoder.transform(np.asarray(X_test['school_state']).reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87398, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_school_state_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 5)\n"
     ]
    }
   ],
   "source": [
    "encoder = one_hot_encoding(X,'teacher_prefix')\n",
    "\n",
    "X_train_teacher_prefix_one = encoder.transform(np.asarray(X_train['teacher_prefix']).reshape(-1,1))\n",
    "X_test_teacher_prefix_one = encoder.transform(np.asarray(X_test['teacher_prefix']).reshape(-1,1))\n",
    "\n",
    "print(X_train_teacher_prefix_one.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 51)\n"
     ]
    }
   ],
   "source": [
    "encoder = one_hot_encoding(X,'clean_categories')\n",
    "\n",
    "X_train_clean_categories_one = encoder.transform(np.asarray(X_train['clean_categories']).reshape(-1,1))\n",
    "X_test_clean_categories_one = encoder.transform(np.asarray(X_test['clean_categories']).reshape(-1,1))\n",
    "\n",
    "print(X_train_clean_categories_one.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 401)\n"
     ]
    }
   ],
   "source": [
    "encoder = one_hot_encoding(X,'clean_subcategories')\n",
    "\n",
    "X_train_clean_subcategories_one = encoder.transform(np.asarray(X_train['clean_subcategories']).reshape(-1,1))\n",
    "X_test_clean_subcategories_one = encoder.transform(np.asarray(X_test['clean_subcategories']).reshape(-1,1))\n",
    "\n",
    "print(X_train_clean_subcategories_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 4)\n"
     ]
    }
   ],
   "source": [
    "encoder = one_hot_encoding(X,'project_grade_category')\n",
    "\n",
    "X_train_project_grade_category_one = encoder.transform(np.asarray(X_train['project_grade_category']).reshape(-1,1))\n",
    "X_test_project_grade_category_one = encoder.transform(np.asarray(X_test['project_grade_category']).reshape(-1,1))\n",
    "\n",
    "print(X_train_project_grade_category_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 51)\n",
      "(87398, 5)\n",
      "(87398, 51)\n",
      "(87398, 401)\n",
      "(87398, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in [X_train_school_state_one,X_train_teacher_prefix_one,X_train_clean_categories_one,X_train_clean_subcategories_one,X_train_project_grade_category_one]:\n",
    "    print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Input 2 , other inputs combined\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "train_others = [X_train_school_state_one,X_train_teacher_prefix_one,X_train_clean_categories_one,\n",
    "                X_train_clean_subcategories_one,X_train_project_grade_category_one,X_train_num_features]\n",
    "\n",
    "X_train_others= hstack(train_others).tocsr()\n",
    "\n",
    "test_others = [X_test_school_state_one,X_test_teacher_prefix_one,X_test_clean_categories_one,\n",
    "               X_test_clean_subcategories_one,X_test_project_grade_category_one,X_test_num_features]\n",
    "\n",
    "X_test_others = hstack(test_others).tocsr()\n",
    "\n",
    "X_train_others = X_train_others.todense()\n",
    "X_test_others = X_test_others.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 514, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#reshaping\n",
    "n_size_train = (X_train_others.shape[0],X_train_others.shape[1],1)\n",
    "X_train_reshaped = np.resize(X_train_others,n_size_train)\n",
    "\n",
    "n_size_test = (X_test_others.shape[0],X_test_others.shape[1],1)\n",
    "X_test_reshaped = np.resize(X_test_others,n_size_test)\n",
    "\n",
    "print(X_train_reshaped.shape)\n",
    "\n",
    "X_Train_consolidated = [X_train_essay,X_train_reshaped]\n",
    "                        \n",
    "X_Test_consolidated = [X_test_essay,X_test_reshaped]\n",
    "\n",
    "for i in X_Train_consolidated:\n",
    "    print(type(i))\n",
    "for i in X_Train_consolidated:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1 input shape of train data: (87398, 514, 1) and test data: (21850, 514, 1) .\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN1 input shape of train data: {} and test data: {} .\".format(n_size_train,n_size_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 1 , essay_input layer\n",
    "\n",
    "input1 = Input(shape=(300,), name='essay_input')\n",
    "embedding_essay_3 = Embedding(input_dim = max_words,output_dim = 300,\n",
    "                            input_length=300 , weights=[embeddings_mat])(input1)\n",
    "\n",
    "reg = l2(0.001)\n",
    "\n",
    "#100 cell LSTM unit\n",
    "lstm_essay_3 = LSTM(100,recurrent_dropout=0.5,kernel_regularizer=l2(0.001),return_sequences=True)(embedding_essay_3)\n",
    "flatten_essay_3 = Flatten()(lstm_essay_3)\n",
    "\n",
    "\n",
    "# input 2\n",
    "input2 = Input(shape=(514,1))\n",
    "cnn_input = Conv1D(filters=64,kernel_size=3,strides=1)(input2)\n",
    "cnn_input = Conv1D(filters=64,kernel_size=3,strides=1)(cnn_input)\n",
    "cnn_input_flatten = Flatten()(cnn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_layer = concatenate([flatten_essay_3, cnn_input_flatten])\n",
    "\n",
    "model_3 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(concat_layer)\n",
    "model_3 = Dropout(0.5)(model_3)\n",
    "model_3 = Dense(200,activation=\"relu\",kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model_3)\n",
    "model_3 = BatchNormalization()(model_3)\n",
    "model_3 = Dropout(0.5)(model_3)\n",
    "model_3 = Dense(80,activation=\"relu\", kernel_initializer=\"glorot_normal\" ,kernel_regularizer=l2(0.001))(model_3)\n",
    "\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output_layer')(model_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "essay_input (InputLayer)        [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 514, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 300)     15468300    essay_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 512, 64)      256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 300, 100)     160400      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 510, 64)      12352       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 30000)        0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32640)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 62640)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          18792300    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          60200       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200)          800         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 80)           16080       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            162         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 34,510,850\n",
      "Trainable params: 34,510,450\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = Model(inputs = [input1,input2],outputs=[output])\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "def custom_auroc(y_true, y_pred):\n",
    "    return tf.py_function(helper, (y_true, y_pred), tf.double)\n",
    "    \n",
    "\n",
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[custom_auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.gitmemory.com/issue/tensorflow/tensorflow/28799/532818343\n",
    "\n",
    "\n",
    "checkpoint_object = ModelCheckpoint('Ass3_Model1.h5',monitor=\"val_custom_auroc\",mode=\"max\",save_best_only = True,verbose=2)\n",
    "\n",
    "\n",
    "earlystop_object = EarlyStopping(monitor = 'val_custom_auroc', mode=\"max\",patience = 4,verbose = 2)\n",
    "\n",
    "logdir = os.path.join(\"logs\",'Ass3_Model1_visualization')\n",
    "tensorboard_object = TensorBoard(log_dir=logdir,histogram_freq=1)\n",
    "\n",
    "callbacks = [checkpoint_object,earlystop_object, tensorboard_object]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.9620 - custom_auroc: 0.5905\n",
      "Epoch 00001: val_custom_auroc improved from -inf to 0.71507, saving model to Ass3_Model1.h5\n",
      "87398/87398 [==============================] - 639s 7ms/sample - loss: 0.9617 - custom_auroc: 0.5905 - val_loss: 0.7234 - val_custom_auroc: 0.7151\n",
      "Epoch 2/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.6228 - custom_auroc: 0.7304\n",
      "Epoch 00002: val_custom_auroc improved from 0.71507 to 0.74726, saving model to Ass3_Model1.h5\n",
      "87398/87398 [==============================] - 631s 7ms/sample - loss: 0.6226 - custom_auroc: 0.7305 - val_loss: 0.5730 - val_custom_auroc: 0.7473\n",
      "Epoch 3/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.5084 - custom_auroc: 0.7737\n",
      "Epoch 00003: val_custom_auroc improved from 0.74726 to 0.75322, saving model to Ass3_Model1.h5\n",
      "87398/87398 [==============================] - 621s 7ms/sample - loss: 0.5082 - custom_auroc: 0.7741 - val_loss: 0.4870 - val_custom_auroc: 0.7532\n",
      "Epoch 4/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.4429 - custom_auroc: 0.8129\n",
      "Epoch 00004: val_custom_auroc did not improve from 0.75322\n",
      "87398/87398 [==============================] - 608s 7ms/sample - loss: 0.4429 - custom_auroc: 0.8129 - val_loss: 0.4825 - val_custom_auroc: 0.7441\n",
      "Epoch 5/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.3863 - custom_auroc: 0.8561\n",
      "Epoch 00005: val_custom_auroc did not improve from 0.75322\n",
      "87398/87398 [==============================] - 611s 7ms/sample - loss: 0.3863 - custom_auroc: 0.8557 - val_loss: 0.4843 - val_custom_auroc: 0.7135\n",
      "Epoch 6/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.3361 - custom_auroc: 0.8877\n",
      "Epoch 00006: val_custom_auroc did not improve from 0.75322\n",
      "87398/87398 [==============================] - 510s 6ms/sample - loss: 0.3361 - custom_auroc: 0.8877 - val_loss: 0.5207 - val_custom_auroc: 0.6920\n",
      "Epoch 7/10\n",
      "87296/87398 [============================>.] - ETA: 0s - loss: 0.3036 - custom_auroc: 0.9127\n",
      "Epoch 00007: val_custom_auroc did not improve from 0.75322\n",
      "87398/87398 [==============================] - 385s 4ms/sample - loss: 0.3036 - custom_auroc: 0.9127 - val_loss: 0.5105 - val_custom_auroc: 0.6774\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model_3.fit(X_Train_consolidated, y_train, \n",
    "                     batch_size=256, epochs=10, verbose=1,\n",
    "                     callbacks=callbacks, validation_data=(X_Test_consolidated,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ass_3 = np.max(model_3.history.history['val_custom_auroc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1b1b3c96f221d58f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1b1b3c96f221d58f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------------------------------------------+------------+\n",
      "|    Features   | Model Name |              Model Specifications              | Custom AUC |\n",
      "+---------------+------------+------------------------------------------------+------------+\n",
      "| Assignment 1  |  Model 1   |      3 dense layers with ReLU activation       | 0.75184745 |\n",
      "|               |            |      Adam Optimizer default learning rate      |            |\n",
      "|               |            |                 Text from Glove                |            |\n",
      "| Assignment 2  |  Model 2   |      3 dense layers with ReLU activation       | 0.7592025  |\n",
      "|               |            |      Adam Optimizer default learning rate      |            |\n",
      "|               |            |                  Text with IDF                 |            |\n",
      "| Assignment 3  |  Model 3   |      3 dense layers with ReLU activation       | 0.75321674 |\n",
      "|               |            |      Adam Optimizer default learning rate      |            |\n",
      "|               |            |  Text from Glove and Conv1D for other features |            |\n",
      "+---------------+------------+------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Please compare all your models using Prettytable library\n",
    "#http://zetcode.com/python/prettytable/\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Features\",\"Model Name\", \"Model Specifications\",\"Custom AUC\"]\n",
    "x.add_row([\"Assignment 1 \", \"Model 1\", \"3 dense layers with ReLU activation \\n Adam Optimizer default learning rate \\n Text from Glove\",'0.75184745'])\n",
    "x.add_row([\"Assignment 2 \",  \"Model 2\",\"3 dense layers with ReLU activation \\n Adam Optimizer default learning rate \\n Text with IDF\",'0.7592025'])\n",
    "x.add_row([\"Assignment 3 \",  \"Model 3\",\"3 dense layers with ReLU activation \\n Adam Optimizer default learning rate \\n Text from Glove and Conv1D for other features\" ,'0.75321674'])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
